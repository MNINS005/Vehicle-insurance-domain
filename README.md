# ğŸš— Vehicle Insurance Prediction â€“ End-to-End MLOps Project

This project is a **complete end-to-end Machine Learning system** that predicts whether a customer will purchase vehicle insurance.  
It covers the full lifecycle:

- Data ingestion from MongoDB  
- Model training & experiment tracking  
- Model versioning and storage in AWS S3  
- Web app deployment using Flask  
- Docker containerization  
- CI/CD pipeline using GitHub Actions  
- Automatic deployment on AWS EC2  

This repository demonstrates a **real-world production ML workflow (MLOps)**.

---

# ğŸ“Œ Problem Statement

Insurance companies need to identify customers who are likely to purchase vehicle insurance.  
The goal of this project is to build a machine learning system that predicts:

ğŸ‘‰ Will the customer respond positively to vehicle insurance?

Output:
- **1 â†’ Response Yes**
- **0 â†’ Response No**

---

# ğŸ§  Tech Stack

## Machine Learning
- Python
- Scikit-learn
- Pandas / NumPy

## MLOps Tools
- DVC â†’ Data & pipeline versioning  
- MLflow â†’ Experiment tracking  
- MongoDB Atlas â†’ Cloud data storage  
- AWS S3 â†’ Model storage  
- Docker â†’ Containerization  
- GitHub Actions â†’ CI/CD  
- AWS EC2 â†’ Deployment  

---

# ğŸ“‚ Project Architecture

Data Source (MongoDB Atlas)
â†“
Data Ingestion Pipeline
â†“
Data Validation
â†“
Data Transformation
â†“
Model Training
â†“
Model Saved to AWS S3
â†“
Flask Web App
â†“
Docker Container
â†“
GitHub Actions CI/CD
â†“
AWS EC2 Deployment



---

# âš™ï¸ Step-by-Step Journey

## 1ï¸âƒ£ Project Setup

Created structured ML project architecture.

src/
â”œâ”€â”€ components/
â”œâ”€â”€ configuration/
â”œâ”€â”€ data_access/
â”œâ”€â”€ cloud_storage/
â”œâ”€â”€ entity/
â”œâ”€â”€ pipeline/
â”œâ”€â”€ exception/
â”œâ”€â”€ logger/



Implemented:
- Custom logging  
- Custom exception handling  
- Constants configuration module  

---

## 2ï¸âƒ£ Data Ingestion from MongoDB Atlas

Connected pipeline to MongoDB using:
- `pymongo`
- `certifi` SSL certificates

Pipeline fetches dataset and converts it into Pandas DataFrame.

---

## 3ï¸âƒ£ Data Validation & Transformation

### Validation
- Schema checks  
- Missing column checks  

### Transformation
Feature engineering + preprocessing using:
- StandardScaler
- ColumnTransformer

Generated features:
- `Vehicle_Age_lt_1_Year`
- `Vehicle_Age_gt_2_Years`
- `Vehicle_Damage_Yes`

Saved artifacts:
artifact/.../preprocessing.pkl


---

## 4ï¸âƒ£ Model Training

Training pipeline performs:
- Train/test split
- Model training
- Evaluation

Saved trained model:
artifact/.../model.pkl


---

## 5ï¸âƒ£ Experiment Tracking (MLflow)

Logged:
- Parameters
- Metrics
- Model versions

Ensures reproducibility and experiment comparison.

---

## 6ï¸âƒ£ Model Storage in AWS S3

Configured AWS CLI + Boto3.

Training pipeline automatically uploads:
- `model.pkl`
- `preprocessing.pkl`

to S3 bucket for production usage.

---

## 7ï¸âƒ£ Prediction Pipeline

Prediction workflow:
1. Load model from S3  
2. Load preprocessor  
3. Accept user input  
4. Return prediction  

---

## 8ï¸âƒ£ Flask Web Application

Converted ML pipeline into web app.

### Routes

| Route | Description |
|---|---|
| `/` | Show prediction form |
| `/train` | Trigger full training |
| POST `/` | Make prediction |

---

## 9ï¸âƒ£ Docker Containerization

Created Docker image for full app.

Added `.dockerignore` to exclude:
- venv
- artifacts
- notebooks
- logs

---

## ğŸ”Ÿ CI/CD with GitHub Actions

### Continuous Integration
On every push:
- Build Docker image
- Push image to AWS ECR

### Continuous Deployment
On EC2 self-hosted runner:
- Pull latest image
- Stop old container
- Run new container automatically

Deployment is fully automated ğŸš€

---

# â˜ï¸ AWS Infrastructure

| Service | Purpose |
|---|---|
| AWS S3 | Model storage |
| AWS ECR | Docker registry |
| AWS EC2 | Application hosting |

---

# ğŸ” GitHub Secrets

AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
AWS_DEFAULT_REGION
ECR_REPO
MONGODB_URL








---

# ğŸš€ Run Locally

### Clone repo
```bash
git clone <repo-url>
cd Vehicle-insurance-domain


#create v-env
python -m venv venv
venv\Scripts\activate
#install dependencies
pip install -r requirements.txt
#run flask
python app.py


### Deployment

# After CI/CD setup, app runs automatically on EC2:
 http://<EC2_PUBLIC_IP>:5000
 